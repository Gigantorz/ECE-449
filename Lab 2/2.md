Employ Multi-layer perceptron (MLP) network. using the backpropagation with momentum learning algorithm, to solve the Wine Dataset classification
Wine is a 3-class problem with 13 predictor attributes
We will use Tensorflow and Keras to build your multilayer perceptron, again using an automated pipeline from data ingestion through parameter exploration (this time using a tenfold cross-validation design) to the final evaluation of your MLP model.

- no missing values in the dataset
The wine dataset is a 3-class problem, the usual practice in neural network modeling is to use a "one-hot" encoding to represent a categorical variable with more than two values, - 
- so your class labels should be one-hot encoded.
- Use a Softmax activation in your last layer to train the network to recognize this encoding.

- randomly split the Wine dataset into a "training dataset" and a holdout testing dataset; 
	- there are 178 examples 
		- reserve 10% as the test set
		- 90% as the train set
Note:
- keep the class frequencies the same in the training and testing datasets, so you will need to use stratified sampling to divide your train and test sets. Functions for doing so are contained in Scikit-Learn.

perform a parameter exploration of the MLP using the tenfold cross-validation design in the training set (again using stratified sampling), and <mark style="background: #FFF3A3A6;">determine the best parameter settings based on the validation error</mark>.
- Then train a fresh MLP using the whole training set and those best parameter settings, and determine the final out-of-sample error using your test dataset.

- [ ] Multi-layer perceptron (MLP)
- [ ] Backpropagation with momentum learning algorithm
- [ ] Tensorflow
- [ ] Keras
- [ ] Softmax activation
- [ ] tenfold cross-validation design
- [ ] automated pipeline
- [ ] Adam Optimizer

Neural Network modeling involves several activities:
- preprocessing the data (including normalization, missing values imputation and encoding)
- network design 
- parameter exploration

Requirements:
To do to the MLP using an automated pipeline.
- ingest,
- preprocess,
- train 
- evaluate

parameter exploration must include both the network design (number of layers, neurons per layer) and the parameters of the learning algorithm
- Suggest using the Adam optimizer,

- evaluate the "best" choice of network design and parameters on the test dataset.
- Report the F1 score of your final experiments.

Tensorflow is not part of Scikit-Learn, so you most likely will not be able to use the “pipeline” container object as you did in Lab #1.
	<mark style="background: #FFF3A3A6;">Instead, you will create your own pipeline by passing the output of one library function to the next, and adding in the control flow needed to organize the tenfold cross-validation experiments.</mark>

---
### Wine Dataset
These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.

### Cross Validation
There is always a need to validate the stability of your machine learning model. You can't fit the model to your training data and hope it would accurately work for the real data it has never seen before.
	You need some kind of assurance that your model has got most of the patterns from the data correct, and its not picking up too much on noise, or in other words its low on bias and variance

Validation
The process of deciding whether the numerical results quantifying hypothesized relationship between variables, are acceptable as descriptions of the data 

residuals
Generally, an error estimation for the model is made after training

training error
- a numerical estimate of the difference in predicted and original responses.

In this process, a numerical estimate of the difference in predicted and original responses is done, also called the training error. However, this only gives us an idea about how well our model does no data used to train it. <mark style="background: #FFF3A3A6;">So, the problem with this evaluation technique is that it does not give an indication of how well the learner will generalize to an independent/unseen data set. Getting this idea bout our model is known as cross validation.</mark>


Holdout Method
basic remedy for this involves removing a part of the training data and using it to get predictions from the model trained on rest of the data. (test data) The error estimation then tells how our model is doing on unseen data or the validation set.
This is a simple kind of cross validation technique
Although this method doesn't take any overhead to compute and is better than traditional validation, <mark style="background: #FFF3A3A6;">it still suffers from issues of high variance. This is because it is not certain which data points will end up in the validation set and the result might be entirely different for different sets.</mark>

K-Fold Cross Validation
As there is enough data to train your model, removing a part of it for validation poses a problem of underfitting. 
	By reducing the training data, we risk losing important patterns/ trends in data set, which in turn increases error induced bias.
		So what we require is a method that provides ample data for training the model and also leaves ample data for validation.

<mark style="background: #FFF3A3A6;">In K Fold cross validation, the data is divided into k subsets. Now the holdout method is repeated k times, such that each time, one of the k subsets is used as the test set/validation set and the other k-1 subsets are put together to form a training set. </mark>
	The error estimation is averaged over all k trials to get total effectiveness of our model. As can be seen, every data point gets to be in validation set exactly once, and gets to be in a training set k-1 times
<mark style="background: #FFF3A3A6;">		This significantly reduces bias as we are using most of the data for fitting, and also significantly reduces variance as most of the data is also being used for validation set.</mark>
Interchanging the training and test sets also adds to the effectiveness of this method. <mark style="background: #FFF3A3A6;">As a general rule and empirical evidence, k = 5, or 10 is generally preferred,</mark> but nothing's fixed and it can take any value

Stratified K-Fold Cross Validation
In some cases, there may be a large imbalance in the response variables. For example, in dataset concerning price of houses, there might be a large number of houses having high price. Or in case of classification, there might be several times more negative samples than positive samples. For such problems, 
	a slight variation in the K fold cross validation is made, such that each fold contains approximately the same percentage of samples of each target class as the complete set, or in case of prediction problems, the mean response value is approximately equal in all the folds.
		- Stratified K Fold

> **Above explained validation techniques are also referred to as Non-exhaustive cross validation methods.** _These do not compute all ways of splitting the original sample, i.e. you just have to decide how many subsets need to be made. Also, these are approximations of_ **method explained below, also called Exhaustive Methods, that computes all possible ways the data can be split into training and test sets.**

## What is Stratified Cross-Validation in Machine Learning
Using Scikit-Learn

#### Stratified Sampling
a sampling technique where the samples are selected in the same proportion (by dividing the population into groups called "Strata" based on a characteristic) as they appear in the population.
Example:
	if the population of interest has 30% male and 70% female subjects, then we divide the population into two ("male" and "female") groups that choose 30% of the sample from the "male" group and 70% of the sample from the "female" group.
	![[1 7qv0fhIqAWzLyGaiekTk9Q.png]]

#### How is stratified sampling related to cross-validation?
Implementing the concept of stratified sampling in cross-validation ensures the training and test sets have the same proportion of the feature of interest as in the original dataset. Doing this with the target variable ensures that the cross-validation result is a close approximation of generalization error.

Implementing hold-out cross-validation without stratified sampling
Hold-out cross validation is implemented using the "train_test_split" method of Scikit-Learn. The Implementation:
```
import pandas as pd
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, StratifiedKFold, StratifiedShuffleSplit, KFold

make_class = make_classification(n_samples=500,n_features=3,n_redundant=0,n_informative=2,n_classes=3,n_clusters_per_class=1,random_state=11)
data = pd.DataFrame(make_class[0],columns=range(make_class[0].shape[1]))
data['target'] = make_class[1]
data.head()
```

Implementing hold-out cross-validation with stratified sampling
implement such that the training and the test sets have same proportion of the target variable. This can be achieved by setting the "stratify" argument of "train_test_split" to the characteristic of interest (target variable, in this case). 
	Doesn't need to be the target variable, it can even be an input variable which you want to have the same proportion in the training and test sets
```
train_df,test_df = train_test_split(data,test_size=0.2,stratify=data['target'],random_state=11)
print(f'PROPORTION OF TARGET IN THE ORIGINAL DATA\n{data["target"].value_counts() / len(data)}\n\n'+
      f'PROPORTION OF TARGET IN THE TRAINING SET\n{train_df["target"].value_counts() / len(train_df)}\n\n'+
      f'PROPORTION OF TARGET IN THE TEST SET\n{test_df["target"].value_counts() / len(test_df)}')
```
Using stratified sampling, the proportion of the target variable is pretty much the same across the original data, training and test set.

- [ ] When would you not want to stratify using the target variable?

Implementing k-fold cross-validation without stratified sampling
K-fold cross-validation splits the data into ‘k’ portions. In each of ‘k’ iterations, one portion is used as the test set, while the remaining portions are used for training. Using the ‘KFold’ class of Scikit-Learn, we’ll implement 3-fold cross-validation without stratified sampling.
```
kfold = KFold(n_splits=3,random_state=11,shuffle=True)
splits = kfold.split(data,data['target']) # each split has a train indexes and test indexes pair
print(f'PROPORTION OF TARGET IN THE ORIGINAL DATA\n{data["target"].value_counts() / len(data)}\n\n')
for n,(train_index,test_index) in enumerate(splits):
    print(f'SPLIT NO {n+1}\nTRAINING SET SIZE: {np.round(len(train_index) / (len(train_index)+len(test_index)),2)}'+
          f'\tTEST SET SIZE: {np.round(len(test_index) / (len(train_index)+len(test_index)),2)}\nPROPORTION OF TARGET IN THE TRAINING SET\n'+
          f'{data.iloc[test_index,3].value_counts() / len(data.iloc[test_index,3])}\nPROPORTION OF TARGET IN THE TEST SET\n'+
          f'{data.iloc[train_index,3].value_counts() / len(data.iloc[train_index,3])}\n\n')
```
The proportion of the target variable is inconsistent among the original data, training data and test data across splits. 

##### Implementing k-fold cross-validation with stratified sampling
Stratified sampling can be implemented with k-fold cross-validation using the ‘StratifiedKFold’ class of Scikit-Learn. The implementation is shown below.
```
kfold = StratifiedKFold(n_splits=3,shuffle=True,random_state=11)
#data['target'] IS THE VARIABLE USED FOR STRATIFIED SAMPLING.
splits = kfold.split(data,data['target'])
print(f'PROPORTION OF TARGET IN THE ORIGINAL DATA\n{data["target"].value_counts() / len(data)}\n\n')
for n,(train_index,test_index) in enumerate(splits):
    print(f'SPLIT NO {n+1}\nTRAINING SET SIZE: {np.round(len(train_index) / (len(train_index)+len(test_index)),2)}'+
          f'\tTEST SET SIZE: {np.round(len(test_index) / (len(train_index)+len(test_index)),2)}\nPROPORTION OF TARGET IN THE TRAINING SET\n'+
          f'{data.iloc[test_index,3].value_counts() / len(data.iloc[test_index,3])}\nPROPORTION OF TARGET IN THE TEST SET\n'+
          f'{data.iloc[train_index,3].value_counts() / len(data.iloc[train_index,3])}\n\n')
```
The proportion of the target variable is pretty much consistent across the original data, training set and test set in all the three splits

Cross validation implemented using stratified sampling <mark style="background: #FFF3A3A6;">ensures that the proportion of the feature of interest is the same across the original data, training set and the test set.</mark> This ensures that no value is over/under represented in the training and test sets, which gives a more accurate estimate of performance/error.