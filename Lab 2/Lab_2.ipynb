{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, train_test_split\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Layer\n",
        "from tensorflow import keras\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from statistics import mean"
      ],
      "metadata": {
        "id": "tLqzm8W1H2Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1qe_58NGcRn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as snsb\n",
        "import collections, functools, operator\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data = pd.read_csv('/content/drive/MyDrive/ECE_449/2/wine.data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghiNgAweXwUd",
        "outputId": "9353a3d7-5a95-43b9-c6ea-50aa94b3cef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a Pipeline\n",
        "# Deprecated\n",
        "class LoadData(BaseEstimator, TransformerMixin):\n",
        "  \"\"\"\n",
        "  This class mounts google drive and reads the data from file to a DataFrame\n",
        "  \"\"\"\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    drive.mount('/content/drive')\n",
        "    X = pd.read_csv('/content/drive/MyDrive/ECE_449/2/wine.data')\n",
        "    return X\n",
        "\n",
        "class MeanImputer(BaseEstimator, TransformerMixin):\n",
        "  \"\"\"\n",
        "    Class to data impute mean values in to rows in columns with no values \"nan\".\n",
        "\n",
        "    ...\n",
        "    Methods\n",
        "    ```````\n",
        "    fit :  returns self, mainly used for fit_transform\n",
        "    transform : Mean data imputes dataset\n",
        "  \"\"\"\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    \"\"\" Assigns names to the columns\n",
        "\n",
        "      Parameters\n",
        "      ``````````\n",
        "      X (DataFrame) : Dataset to be data imputed\n",
        "\n",
        "      Returns\n",
        "      ```````\n",
        "      X (DataFrame) : data imputed Dataset\n",
        "    \"\"\"\n",
        "    X = X.fillna(X.mean())\n",
        "    return X\n",
        "\n",
        "class FormatClass(BaseEstimator, TransformerMixin):\n",
        "  \"\"\"\n",
        "    Class to give the columns of the dataset names\n",
        "\n",
        "    ...\n",
        "    Methods\n",
        "    ```````\n",
        "    fit :  returns self, mainly used for fit_transform\n",
        "    transform : will assign names to columns.\n",
        "  \"\"\"\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "\n",
        "  def transform(self, X):\n",
        "    \"\"\" Assigns names to the columns\n",
        "\n",
        "      Parameters\n",
        "      ``````````\n",
        "      X (DataFrame) : Dataset with columns to be renamed\n",
        "\n",
        "      Returns\n",
        "      ```````\n",
        "      X (DataFrame) : renamed columns in Dataset\n",
        "    \"\"\"\n",
        "    column_names = [\"Class\", \"Alcohol\", \"Malic Acid\", \"Ash\", \"Alcalinity of Ash\", \"Magnesium\", \"Total Phenols\",\n",
        "                \"Flavanoids\", \"Nonflavanoid Phenols\", \"Proanthocyanins\", \"Color Intensity\", \"Hue\", \"OD280/OD315 of diluted wines\", \"Proline\"]\n",
        "    X.columns = column_names\n",
        "    return X\n",
        "\n",
        "class Scaler(BaseEstimator, TransformerMixin):\n",
        "  \"\"\"\n",
        "    Class to MinMax Scale the dataset except for the target values\n",
        "\n",
        "    ...\n",
        "    Methods\n",
        "    ```````\n",
        "    fit : returns self, mainly used for fit_transform\n",
        "    transform : will MinMax Scale the desired columns in dataset.\n",
        "  \"\"\"\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    \"\"\" MinMax scales the dataset except for the target columns\n",
        "\n",
        "      Parameters\n",
        "      ``````````\n",
        "      X (DataFrame) : Dataset to be scaled\n",
        "\n",
        "      Returns\n",
        "      ```````\n",
        "      X (DataFrame) : Scaled Dataset\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    X.loc[:,[\"Alcohol\", \"Proline\"]] = scaler.fit_transform(X.loc[:,[\"Alcohol\", \"Proline\"]])\n",
        "    return X\n",
        "\n",
        "class FeatureEncoder(BaseEstimator, TransformerMixin):\n",
        "  \"\"\"\n",
        "    Class to One-hot-encode the dataset\n",
        "\n",
        "    ...\n",
        "    Methods\n",
        "    ```````\n",
        "    fit : returns self, mainly used for fit_transform\n",
        "    transform : will encode \"Class\" column of the dataset\n",
        "\n",
        "  \"\"\"\n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    \"\"\" Creates a OneHotEncoder object, separates the X and y from the given\n",
        "        dataset then the y is OneHotEncoded into 'Class_1', 'Class_2', and 'Class_3'\n",
        "        then joins them back together and returns the joined dataset.\n",
        "\n",
        "    Parameters\n",
        "    ``````````\n",
        "    X (DataFrame) : Dataset to be one-hot encoded.\n",
        "\n",
        "    Returns\n",
        "    ```````\n",
        "    X (DataFrame) : One-hot encoded dataset\n",
        "    \"\"\"\n",
        "    encoder = OneHotEncoder(sparse_output = False)\n",
        "\n",
        "    # Converting type of columns to category\n",
        "    X[\"Class\"] = X[\"Class\"].astype('category')\n",
        "\n",
        "    X_num = X.select_dtypes(exclude='category')\n",
        "    X_cat = X.select_dtypes(include='category')\n",
        "    df_new = pd.get_dummies(X_cat, columns = [\"Class\"], prefix = \"Class\")\n",
        "    column_name = df_new.columns\n",
        "    encoded_cat = encoder.fit_transform(X_cat)\n",
        "    one_hot_features = pd.DataFrame(encoded_cat, columns = column_name)\n",
        "    X = X_num.join(one_hot_features)\n",
        "    return X\n",
        "\n",
        "class argmax_layer(Layer):\n",
        "  \"\"\"\n",
        "    Class to create Argmax_layer on the MLP\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(argmax_layer, self).__init__()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.math.argmax(inputs, axis=1)"
      ],
      "metadata": {
        "id": "qQtzay1J9KDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(BaseEstimator, TransformerMixin):\n",
        "  \"\"\"\n",
        "  Creates and trains classifier on the Stratified K Fold\n",
        "\n",
        "  ...\n",
        "  Methods\n",
        "  ```````\n",
        "  make_model:\n",
        "    makes desired Sequential model\n",
        "  evaluate:\n",
        "    Evaluates model\n",
        "  fit:\n",
        "    To fit the dataset with model and to find the optimal model\n",
        "  transform:\n",
        "    returns model\n",
        "  \"\"\"\n",
        "\n",
        "  # neuron must be the same length as the layer and the first index is the highest layer.\n",
        "  def make_model(self, layer = 1, neuron = [], lr = 0.0):\n",
        "    \"\"\" Makes a model with the given values of layers, neurons, and learning rate.\n",
        "        The last layer will have 3 neurons and will have softmax activation on the output layer\n",
        "        and also argmax to get one val for one hot encoded.\n",
        "\n",
        "        Return: returns a made model\n",
        "    Parameters\n",
        "    ``````````\n",
        "    layer : int, default = 1\n",
        "             decides how many layers the model will have\n",
        "\n",
        "    neuron : int list, default = []\n",
        "            list of neuron values. The first index will be the outer layer\n",
        "            neuron, then the next index will be the next layer and the last value\n",
        "            will be the last neuron.\n",
        "            The max length of the list can only be 3.\n",
        "\n",
        "    lr : float64, default = 0.0\n",
        "          Value for the learning rate of the model\n",
        "\n",
        "    Returns\n",
        "    ```````\n",
        "    model (Sequential) : Model made with the parameters given, with softmax activation layer\n",
        "                          and argmax.\n",
        "    ``````\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    if layer >= 1:\n",
        "      # decide if layer 1 or layer 2, then go through the neurons\n",
        "      if layer == 2:\n",
        "        print(\"2 layer and we are adding neurons {}, and {}\".format(neuron[0], neuron[1]))\n",
        "        model.add(Dense(neuron[0], input_dim = 8, activation = 'relu'))\n",
        "        model.add(Dense(neuron[1], activation = 'relu'))\n",
        "      else:\n",
        "        print(\"1 layer and we are adding neuron = {}\".format(neuron[0]))\n",
        "        model.add(Dense(neuron[0], input_dim = 8, activation = 'relu'))\n",
        "\n",
        "    print(\"Making output layer and compile\")\n",
        "    model.add(Dense(3, activation = \"softmax\"))\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    # adding argmax to the model to spit out one value\n",
        "    # https://stackoverflow.com/a/72850239\n",
        "    # model.add(Lambda(lambda x: k.cast(k.argmax(x), dtype = 'int64'), name = 'y_pred'))\n",
        "    # or\n",
        "    model.add(argmax_layer())\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "  def evaluate(self, model, data):\n",
        "    \"\"\" Will evaluate the performance of MLP model and the stratified K Fold in\n",
        "        a range of k values.\n",
        "\n",
        "    Parameters\n",
        "    ``````````\n",
        "    model (Sequential): MLP model we are evaluating in.\n",
        "    data (DataFrame): Non-one hot encoded dataset.\n",
        "\n",
        "    Returns\n",
        "    ````````\n",
        "    mean_results (float) : returns the mean scores of the test of different k\n",
        "                          params for StratfiedKFold on the model\n",
        "    results_kfold_mean (dict) : mean of all the scores from kfold test on the model\n",
        "    \"\"\"\n",
        "    # Data can't be one hot encoded\n",
        "    # X = data.loc[:,'Alcohol' : 'Proline']\n",
        "    # y = data.loc[:, 'Class_1' : 'Class_3']\n",
        "    y = data[\"Class\"]\n",
        "    skf = StratifiedKFold()\n",
        "    results_kfold_mean = {}\n",
        "    result = []\n",
        "\n",
        "    kFold_range = list(range(2,10))\n",
        "    param_grid = dict(kFold__n_splits = range(2, 10))\n",
        "\n",
        "    fold = range(3, 10)\n",
        "    list_scores = []\n",
        "\n",
        "    skf = StratifiedKFold()\n",
        "\n",
        "    pipe = Pipeline([\n",
        "      (\"encoder\", FeatureEncoder()),\n",
        "      (\"scale\", Scaler())\n",
        "    ])\n",
        "\n",
        "    temp_data = pipe.fit_transform(data)\n",
        "    temp_data.info()\n",
        "    X = temp_data.loc[:,'Alcohol' : 'Proline']\n",
        "    y = temp_data.loc[:, 'Class_1' : 'Class_3']\n",
        "    y_temp = y\n",
        "\n",
        "    for k in kFold_range:\n",
        "      skf = StratifiedKFold(n_splits = k)\n",
        "      StratifiedKFold()\n",
        "      scores = []\n",
        "      \"\"\"\n",
        "      ERROR: ValueError: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead.\n",
        "\n",
        "      I think the issue is because of the fact that Stratified K fold cannot take a dataset with more than 1 columns\n",
        "      as a target dataset.\n",
        "        This issue has halted my progress in finishing this lab\n",
        "\n",
        "        Steps taken to fix issue:\n",
        "        - I have googled and stackoverflowed this problem and cannot find a solution.\n",
        "        - I have emailed lab instructor about issue, and got no response\n",
        "        - I have emailed the lab TA Amir Noohian and have gotten a possible solution\n",
        "          - Implement argmax to MLP\n",
        "            - I have implemented argmax to MLP just to find that that wasn't the issue.\n",
        "              - I discovered the issue is because of k fold not being able to partition\n",
        "                my one-hot encoded target values.\n",
        "\n",
        "      I am not at the end of my lab project. I can no longer continue from here.\n",
        "      I hope I am able to get partial marks for the effort that I have put into this lab\n",
        "        I really tried my best and this is my best.\n",
        "      \"\"\"\n",
        "      for train, test in skf.split(X, y):\n",
        "        print(\"\\nThis is the y value with train index, {} and y_train\".format(y[train], scores))\n",
        "      # Fit the model\n",
        "        model.fit(X[train], y_temp[train], epochs = 150, batch_size = 10, verbose = 0)\n",
        "        scores = model.evaluate(X[test], y_temp[test], verbose = 0)\n",
        "        list_scores.append(scores[1] * 100)\n",
        "      results_kfold_mean[str(k)] = mean(scores)\n",
        "\n",
        "    mean_results = mean(sum(results_kfold_mean.values()))\n",
        "    return mean_results, results_kfold_mean\n",
        "\n",
        "  def optimal(self, X):\n",
        "    \"\"\"\n",
        "      Finds optimal model and K value of stratified K Fold\n",
        "\n",
        "      Parameters\n",
        "      ``````````\n",
        "      X (DataFrame) : dataset to train the model with\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    n_cols = X.shape[1] - 3\n",
        "\n",
        "    num_layers = [1,2,3]\n",
        "    neurons_val = [32,64,128]\n",
        "    learning_rate_arr = [0.001, 0.01, 0.1]\n",
        "\n",
        "    # The combination with the highest mean scores will be the optimal model\n",
        "    # mean_result (key) : combo list (value)\n",
        "    mean_results_dict = {}\n",
        "\n",
        "    # Will merge all the collected values then add all the same keys\n",
        "    # The key with the max value will be the ideal K value\n",
        "    results_kfold_mean = {}\n",
        "\n",
        "    # Parameter Exploration\n",
        "    for layer in num_layers:\n",
        "      for i in range(0, len(neurons_val)-1):\n",
        "        for rate in learning_rate_arr:\n",
        "          # Trying different neuron vals.\n",
        "          neuron = []\n",
        "          for j in range(i, -1, -1):\n",
        "            neuron.append(neurons_val[j])\n",
        "          if len(neuron) < layer:\n",
        "            for k in range(0, layer - len(neuron)):\n",
        "              neuron.append(max(neuron))\n",
        "          print(neuron)\n",
        "          model = self.make_model(layer = layer, neuron = neuron, lr = rate)\n",
        "          mean_results, kfold_dict = self.evaluate(model, X)\n",
        "          combo = [layer, neuron, rate]\n",
        "          mean_results_dict[mean_results] = combo\n",
        "\n",
        "          results_kfold_mean = {**results_kfold_mean, **kfold_dict}\n",
        "\n",
        "    print(\"init dict: \", str(results_kfold_mean))\n",
        "    result = dict(functools.reduce(operator.add, map(collections.Counter, results_kfold_mean)))\n",
        "    print(\"final dict: \", str(result))\n",
        "\n",
        "    optimal = 0\n",
        "    for key in mean_results_dict:\n",
        "      if key > optimal:\n",
        "        optimal = key\n",
        "\n",
        "    arr_optimal = mean_results_dict[optimal]\n",
        "    ideal_model = self.make_model(layer = arr_optimal[0], neuron = arr_optimal[1], lr = arr_optimal[2])\n",
        "    ideal_model.summary()\n",
        "    return ideal_model\n",
        "\n",
        "  def fit(self, X, y=None):\n",
        "    model = Sequential()\n",
        "    model = self.optimal(X)\n",
        "\n",
        "    X_num = X.loc[:,'Alcohol' : 'Proline']\n",
        "    y = X.loc[:, 'Class_1' : 'Class_3']\n",
        "    model.fit(X_num, y)\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    return X"
      ],
      "metadata": {
        "id": "bbIYFC-1bEW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame()\n",
        "load = LoadData()\n",
        "data = load.fit_transform(data)\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"imputer\", MeanImputer()),\n",
        "    (\"format\", FormatClass()),\n",
        "    (\"mlp\", Classifier())\n",
        "])\n",
        "\n",
        "pipe.fit_transform(data)"
      ],
      "metadata": {
        "id": "TqpgxKha_GET",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8fdb008b-5df9-48a3-c1e7-c8f8d5a5a50c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[32]\n",
            "1 layer and we are adding neuron = 32\n",
            "Making output layer and compile\n",
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_14 (Dense)            (None, 32)                288       \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 3)                 99        \n",
            "                                                                 \n",
            " argmax_layer_7 (argmax_lay  (None,)                   0         \n",
            " er)                                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 387 (1.51 KB)\n",
            "Trainable params: 387 (1.51 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 177 entries, 0 to 176\n",
            "Data columns (total 16 columns):\n",
            " #   Column                        Non-Null Count  Dtype  \n",
            "---  ------                        --------------  -----  \n",
            " 0   Alcohol                       177 non-null    float64\n",
            " 1   Malic Acid                    177 non-null    float64\n",
            " 2   Ash                           177 non-null    float64\n",
            " 3   Alcalinity of Ash             177 non-null    float64\n",
            " 4   Magnesium                     177 non-null    int64  \n",
            " 5   Total Phenols                 177 non-null    float64\n",
            " 6   Flavanoids                    177 non-null    float64\n",
            " 7   Nonflavanoid Phenols          177 non-null    float64\n",
            " 8   Proanthocyanins               177 non-null    float64\n",
            " 9   Color Intensity               177 non-null    float64\n",
            " 10  Hue                           177 non-null    float64\n",
            " 11  OD280/OD315 of diluted wines  177 non-null    float64\n",
            " 12  Proline                       177 non-null    float64\n",
            " 13  Class_1                       177 non-null    float64\n",
            " 14  Class_2                       177 non-null    float64\n",
            " 15  Class_3                       177 non-null    float64\n",
            "dtypes: float64(15), int64(1)\n",
            "memory usage: 22.2 KB\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-123082a3b377>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# data.info()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-7f4613516945>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    180\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mX_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Alcohol'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m'Proline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-7f4613516945>\u001b[0m in \u001b[0;36moptimal\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    158\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m           \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneuron\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m           \u001b[0mmean_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkfold_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m           \u001b[0mcombo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0mmean_results_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmean_results\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-7f4613516945>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, data)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nThis is the y value with train index, {} and y_train\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m       \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    350\u001b[0m             )\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mtrain_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_test_masks\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iter_test_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0mtest_folds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtest_folds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_make_test_folds\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mallowed_target_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype_of_target_y\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_target_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    677\u001b[0m                 \"Supported target types are: {}. Got {!r} instead.\".format(\n\u001b[1;32m    678\u001b[0m                     \u001b[0mallowed_target_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_of_target_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On cell 5 line 113\n",
        "\n",
        "https://colab.research.google.com/drive/1X-6j4YCjPddry6k8yIhXhjycvgVoWlTC#scrollTo=bbIYFC-1bEW-&line=113&uniqifier=1\n",
        "\n",
        "---\n",
        "ERROR: ValueError: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead.\n",
        "\n",
        "I think the issue is because of the fact that Stratified K fold cannot take a dataset with more than 1 columns\n",
        "as a target dataset.\n",
        "  This issue has halted my progress in finishing this lab\n",
        "  \n",
        "  Steps taken to fix issue:\n",
        "  - I have googled and stackoverflowed this problem and cannot find a solution.\n",
        "  - I have emailed lab instructor about issue, and got no response\n",
        "  - I have emailed the lab TA Amir Noohian and have gotten a possible solution\n",
        "    - Implement argmax to MLP\n",
        "      - I have implemented argmax to MLP just to find that that wasn't the issue.\n",
        "        - I discovered the issue is because of k fold not being able to partition\n",
        "          my one-hot encoded target values.\n",
        "\n",
        "I am not at the end of my lab project. I can no longer continue from here.\n",
        "I hope I am able to get partial marks for the effort that I have put into this lab\n",
        "  I really tried my best and this is my best.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_8D1rRldIrc"
      }
    }
  ]
}