 Using the Scikit-learn libraries, build an AI pipeline that preprocess 

AI pipeline is the main structure.
Everything but AI. 

K-nn 
K is the parameter exploration
How many inputs am I taking
Test set of the classifier will vary depending on the value of K. 

When you are a machine learning Engineer, you are not the boss. You have to present your findings to the boss that your findings are accurate and convince the boss that you have the right answer.
Make a persuasive case that you have found the right value of K in the exercise.

Presentation 
- how well and you present the case of the data set.
	- helps you understand what the different attributes are.

---
Notes
Numpy,
- Numerical Python,
- essential open source library for Python.

Provides support for large, multi-dimensional arrays and matrices
Consists of an assortment of high-level mathematical functions to operate on these arrays
It forms the foundation for various data manipulation and analysis tasks.

Pandas
- open source data manipulation library for python
- simplifies data handling, and cleaning and analysis, making it a preferred choice for data professionals 
	- of data and getting ready for handling
- It bridges the gap between data sources and data analysis, making it easy to import. manipulate and analyze data.

- The primary data structure in NumPy is the ndarrray
	- n-dimensional array object
		- powerful tool for scientific computing.

Matplotlib
- open source library for creating static animated, and interactive visualizations in Python.
- One of the most widely used data visualization libraries due to its flexibility and extensive community support.

Seaborn
- powerful python data visualization library built on top of matplotlib
- designed to be user-friendly and intuitive 
- offers a high-level interface for creating various statistical plots with just few lines of code.
- An excellent choice for both beginners and experienced data scientists.

- similar to matplotlib
- user-friendly and intuitive\
-
Scikit-learn
- provides a simple and efficient tool for data analysis and modeling
- built on top of other popular Python libraries like NumPy, SciPy and Matplotlib

Import libraries
Don't need to install anything for google collabe the libraries that we are working with today won't need to be installed as It comes with already with Google collab.

---
You should begin processing this dataset by looking at the basic statistics of each attribute (median, 1st and 3rd
quartile, min, max); this is best visualized as side-by-side box and whisker plots for each attribute.
Matplotlib / Seaborn can be used to programmatically generate such a plot

Check for missing data by looking median, 1st and 3rd
Quartile, min, max.
Best visualized as box and whisker plots for each attribute (matplotlib, seaborn)
- Scale data using min-max scaling (from sklearn.preprocessing import MinMaxScaler)
- split data to train and test (from sklearn.model)selection import train_test_splot)
- Build an AI pipeline for K-nearest neigbours (k-NN) classifier.
- Your pipeline must automatically explore the performance of the k-NN algorithm for different values of k in (1,10).
- GridsearchCV can be used for this automatic exploration.
USe F1 score for performance evaluation.
### Ask google how to load data set using pandas.

### Also google how to visualize using box and whisker plots using matplot lib


Certain values in these "predictor" attributes are missing and must be filled in.
	Use mean imputation
		- replace each missing value with the mean value of that attributed within the same class.
Rescale each attribute of your data to the interval [0,1] using the min-max scaling technique and the min and max values you found previously. 
Transform your class labels to 0 for benign and 1 for malignant
finally
	randomly split your dataset into a training and testing set (preserver your class frequencies using stratified sampling).
Make sure data is shaped properly for the learning algorithm

Train a k-Nearest Neighbours (k-NN) classifier to model this data set
- Simple algorithm with a single hyperparameter (k, the number of nearest neighbours to consider.)
- The model predicts that the class of datapoint is the same as the class for the majority of the k datapoints closest it in feature space.
- Your pipeline must automatically explore the performance of the k-NN algorithm for different values of k 
	- K -> 1 - 10 IS ENOUGH
	- <mark style="background: #FFF3A3A6;">Compare the performance of the different k-NN versions on the test dataset using the F1 score, and report the performance of each version and your final conclusion on the best choice of the parameter k.</mark>
		- matlplotlib or seaborn
			- in a corporate or research lab environment, you will need to persuade the actual decision-makers that your models were correct. 
				- Convince me!
					- 
# Imputation (Statistics)
- process of replacing missing data with substituted values.
Unit imputation
- subbing for a data point
Item imputation
- subbing for a component of a data point.

Three main problems that missing data causes:
1. can introduce a substantial amount of bias
2. make the handling and analysis of the data more arduous
3. Create reductions in efficiency.

Missing data can create problems for analyzing data, imputation is seen as a way to avoid pitfalls involved with listwise deletion of cases that have missing values.
Is more than one values are missing for a case, most statistical packages default are discarding any case that has a missing value.  
Imputation preserves all cases by replacing missing data with an estimated value based on other available information.
Once all missing values have been imputed, the data can then be analysed using standard techniques for complete data.

# Precision and recall
precision and recall are performance metrics that apply data retrieved from a collection or sample space.
### Precision (positive predictive value)
- fraction of relevant instances among the retrieved instances.
$precision = \frac{relevant\_retrieved\_instances}{all\_retrieved_\_instances}$

### Recall (Sensitivity)
- fraction of relevant instances that were retrieved
$precision = \frac{relevant\_retrieved\_instances}{all\_relevant\_instances}$

Example:
Consider a computer programming of recognizing dogs 
in a picture there are 10 cats and 12 dogs
The program recognizes 8 dogs.
Only 5 are actually dogs while the other 3 are cats 
7 dogs were missed, and 7 cats were correctly excluded.
The programs precision: ($\frac{true\_positives}{selected\_elements}$)
	5/8
The programs recall: ($\frac{true\_positives}{relevant\_elements}$)
	5/12

In a classification task, 
	the precision for a class
	is the number of true positives 
		(# of times correctly labelled as belonging to the positive class)
	divided by the total number of elements labelled as belonging to the positive class 
		(the sum of true positives and false positives)
			items incorrectly labelled as belonging to the class

Recall in this context is defined 
	as the number of true positives divided by the total number of elements that actually belong to the positive class 
		- the sum of true positives and false negatives, which are items that were not labelled as belonging to the positive class but should have been

---
We need a target variable and we need predictor variables.
X - predictor 
y - target variable. (Pop)

what is the predictor variable of our data?

Lab 1

- Scale your data using min-max scaling (from sklearn.preprocessing import MinMaxScaler) ➔ Add this scaler to the pipeline
- Add k-Nearest Neighbors (k-NN) classifier to the pipeline
- randomly split the dataset into a training and testing set (preserve your class frequencies using stratified sampling)
    - how do we make sure that our data is shaped properly for the learning algorithm?

Goal of the model is to predict that the class of a test datapoint is the same as the class of the majority of the datapoints closest to it in feature space.

- Your pipeline must automatically explore the performance of the k-NN algorithm for different values of k in (1,10).
- GridSearchCV can be used for this automatic exploration.
- Use F1 score for performance evaluation
- Find the best choice of the parameter k

Stratified sampling

- when splitting a datset into training and testing sets, it is crucial to ensure that both gets accurately represent the original distribution of classes. stratify parameter
- splits the dataset in a manner that preserves the original class distribution in both the training and testing sets. By using stratified sampling, we can ensure that our models are trained and evaluated on a representative sample of the original dataset. helps prevent issues such as overfitting the majority class or underestinating the importance of minority class.