### [[Convolutional Neural Networks]] (CNN)
- build to invariance to certain variations shifting, illumination....
- Deal with very high dimensional inputs without the need of large number of parameters
- Instead of learning to detect the whole image, detect smaller fragments in the image.

![[Pasted image 20231010220453.png]]
![[Pasted image 20231010220502.png]]
- understanding that these two images are two different instances (i.e. 'X') of the same concept is difficult for a simple neural network.

![[Pasted image 20231010220555.png]]
- a *parameter matrix* is called a filter
- we need three filters to detect three curves

![[Pasted image 20231010220731.png]]
- *Align the filter and the image patch*.
	- Multiply each image pixel by the corresponding filter pixel.
		- add the result
			- divide by the size of the filter. (Implementation differs)

##### Activation Map
![[Pasted image 20231010221326.png]]
9x9 convolution on a 3x3 = 7x7 activation map
![[Pasted image 20231010221441.png]]

![[Pasted image 20231010221450.png]]
Stride is the distance between two consecutive image patches during convolution
Stride is measured in terms of pixels

If stride < patch - width
- patches overlap

If stride >= patch - width
- patches do not overlap

Pad additional pixels on the boundary of the input to achieve a specific output size.
Ways of padding
- pad pixels with value equal to zero
- Repeat the border pixels
- Reflect the image around the border

![[Pasted image 20231010221812.png]]

![[Pasted image 20231010221848.png]]
- The extent of the local region connected to one hidden unit is known as receptive field
- **To increase the receptive field without increasing the number of parameters, use dilation**

![[Pasted image 20231010221953.png]]
- pool hidden units in a non-overlapping neighbourhood in a single channel.
Pooling functions:
- max, min, weighted average, etc.

Pooling introduces invariance to local translations
> Max pooling achieves partial invariance to small translations because the max of a region depends only on the single largest element. If a small translation doesn’t bring in a new largest element at the edge of the pooling region and also doesn’t remove the largest element by taking it outside the pooling region, then the max doesn’t change.
> 
> Imagine a picture of pixel 10x10. And there is a one-pixel nose at (4,5) and one-pixel mouth at (6,5). I define a pattern that a mouth beneath a nose compose a face. Maxpooling maps the 10x10 picture to a 5x5 picture. Thus the nose at (4,5) is mapped to (2,3) and the mouth at (6,5) is mapped to (3,3) in the smaller picture. Now I take another picture with a one-pixel nose at (4,6) and one-pixel mouth at (6,6), i.e. a translation of one pixel to the right. By using Maxpooling, they are mapped to (2,3) and (3,3), which is still classified at a face. And this ability is called “translation invariance”.

>Actually, maxpooling not only creates translation invariance, but also - in a larger sense - deformation invariance.

>And that’s why it is important in convolution networks.

This is what Lab 3 should look like
![[Pasted image 20231010222113.png]]
```
from tensorflow.keras import layers, models
model = models.Sequential()
#define filters and convolutional layers here
model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu',
input_shape=(28, 28, 1)))
#Add a maxpooling layer
model.add(layers.MaxPooling2D(pool_size=(2, 2)))
#Flatten the output and give it to a fully connected layer
model.add(layers.Flatten())
#one hidden layer maps the flattened neurons to output
model.add(layers.Dense(10, activation='softmax’))
model.compile(optimizer='adam', loss='categorical_crossentropy',
metrics=['accuracy']) 
```

Notes from the above Code:
# Conv2D
- specifies the type of layer being added, which is a 2D convolutional layer.

Convolutional layers are fundamental building blocks in CNNs and are particularly effective for image-related tasks.

Filters = 16
- defines the number of filters (or kernals) in the convolutional layer.

Filters are small grids that move across the input data to detect specific patterns. In this case, there are 16 filters.
  Each filter learns to recognize different features in the input data.

Kernal_size = (3, 3)
- sets the size of each filter. In this case, each filter is a 3x3 grid. During the convolution operation, this 3x3 filter slides (or convolves) across the input data to detect patterns.

activation = 'relu'
- after the convolution operation, an activation function is applied element-wise to the output of convolution.
[[ReLU]]
  - introduces non-linearity by replacing all negative values in the output with zero.
    This helps the network learn complex patterns in the data.

input_shape= (28, 28, 1)
- specifies the shape of the input data that will be fed into this layer.
  - In this case, it indicates taht the input data consists of 28x28 pixels and has a single channel

For MNIST images which are grayscale, there is only one channel.
IF the image were RGB color images, the input shape would be '(28, 28, 3)' because there would be three color channels (r,g,b)

The output shape of a Conv2D layer in a neural network depends on several factors, including the input shape, filter size, padding, and strides. In the given code `model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))`, assuming no padding and default stride (which is 1), the output shape can be calculated using the following formula:

$Output shape=(\frac{input\ height−kernel\ height+1}{stride})×(\frac{input\ width−kernel\ width+1}{stride})×number\ of\ filters$

Given:

- Input shape: 28×28×1 (height × width x channels)
- Kernel size: 3×3
- Number of filters: 16
- Stride: 1 (default)

Plugging in the values:

Output height=($\frac{28 - 3 + 1}{1}$)×16=26×16=416
Output width=($\frac{28 - 3 + 1}{1}$)×16=26×16=416

So, the output shape of the Conv2D layer in this case would be 26×26×16. Each of the 16 filters produces a 26×26 feature map.
# Max-Pooling Layer
- is a down-sampling operation commonly used in convolutional neural networks (CNNs) for reducing the spatial dimension of an image or a feature map. It helps in decreasing the computational complexity and controlling overfitting by reducing the number of parameters in the network.

layers.MaxPooling2D
- this specifies the type of layer being added, which is a 2d max-pooling layer.
  Max-pooling is done independently for every depth slice of the input. It helps retain the most information from each feature map while reducing dimensions

pool_size = (2,2)
- defines the size of the pooling window. in this case 2x2 window.
  - during the max-pooling operation, the layer selects the max value from each 2x2 window in the input feature map and retains only those values, discarding the rest.
    - This reduces the spatial dimensions of the input by a factor of 2 along each dimension

After a convolutional layer detects various features in different parts of the image, max-pooling helps in consolidating and summarizing this information.
- It keeps the most significant features (the max values) while discarding less relevant information, leading to a more compact representation.
# Flatten
In a convolutional neural network (CNN), convolutional and pooling layers are typically followed by one or more fully connected layers. However, these fully connected layers require one-dimensional input, whereas the output of convolutional and pooling layers is two-dimensional (or higher-dimensional if there are more than one channel).

The flatten layer resolves this mismatch in dimensions. It takes the multi-dimensional output from the preceding layer and transforms it into a one-dimensional array, effectively flattening the input. This one-dimensional array can then be fed into the subsequent fully connected layers for further processing.

MNIST dataset: a large database of handwritten digits that is
commonly used for training various image processing systems.
●Images are 28 * 28 pixels 
```
from tensorflow.keras.datasets import mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()
```
![[Pasted image 20231010222502.png]]

Convert the class labels into a one-hot encoding
Use the following:
```
from tensorflow.keras.utils import to_categorical
y_train_encoded = to_categorical(y_train)
y_test_encoded = to_categorical(y_test)
```

You will solve an MNIST Classification task using CNN
[tutorial](https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/)

- Use a ReLU activation instead of a sigmoid one
- In your pooling layers, use the MAX() function instead of the arithmetic mean
- For Classification, use a single dense layer followed by a softmax layer

Use stratified 5-fold cross validation to test the performance of
the models for at least the following parameters:
```
Number of Filters=[16, 32]
Learning rate = [0.001, 0.01]
```

Once you have determined the best design, train the CNN one
more time on the entire training set.

Report your performance on the out-of-sample test set.

You have 60,000 training images with 28 * 28 features, so your
training will take a lot of time ➔ Over one hour!

---
### Article Readings:
[How to Develop a CNN for MNIST Handwritten Digit Classification by Jason Brownlee](https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/)
in order to estimate the performance of a model for a given training run, we can further split the training set into a train and validation set.
	Performance on the train and validation dataset over each run can be plotted to provide learning curves and insight into how well a model is learning the problem.
	- we are doing this by using 5-fold cross validation.

Loading Dataset
We know:
- that the images are all pre-aligned
- the images have the same square size of 28x28 pixels 
- images are grayscale

We can load the images and reshape the data arrays to have a single color channel
```
# reshape dataset to have a single channel
trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
testX = testX.reshape((testX.shape[0], 28, 28, 1))
```

Prepare Pixel Data
We know that the pixel values for each image in the dataset are unsigned integers int he range between black and white, or 0 and 255.
- We don't know the best way to scale the pixel values for modeling, but we know that some scaling will be required.
	- A good starting point is to [normalize the pixel values](https://machinelearningmastery.com/how-to-normalize-center-and-standardize-images-with-the-imagedatagenerator-in-keras/) of grayscale images, e.g. rescale them to the range [0,1]. This involves first converting the data type from unsigned integers to floats, then dividing the pixel values by the maximum value.

Evaluating the model
```
# evaluate a model using k-fold cross-validation
def evaluate_model(dataX, dataY, n_folds=5):
	scores, histories = list(), list()
	# prepare cross validation
	kfold = KFold(n_folds, shuffle=True, random_state=1)
	# enumerate splits
	for train_ix, test_ix in kfold.split(dataX):
		# define model
		model = define_model()
		# select rows for train and test
		trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]
		# fit model
		history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)
		# evaluate model
		_, acc = model.evaluate(testX, testY, verbose=0)
		print('> %.3f' % (acc * 100.0))
		# stores scores
		scores.append(acc)
		histories.append(history)
	return scores, histories
```

---
# Lab Results
We are supposed to make our models based on the given code from the slides said Amir, and I have made a model and trained them with the given CNN code. 
- But with my previous implementation of method evaluate, I was only returning the hyperparameters of the optimized model.
	- I have the compiled model with the hyperparameters of optimized model saved in my local disk but I still need to train it smh.
		- the optimized values are filters = 32, learning rate = 0.001.
			- need to train again.
				- my cpu was dying
					- Try using Cuda!!!
![[Pasted image 20231015125326.png]]
![[Pasted image 20231015212832.png]]

---
From Google Collab
# Convolutional Neural Networks (CNN)
Align the filter and the image patch is when you get activation
Stride
- distance between two consecutive image patchs used during convolution.

Padding
- pad additional pixels on the boundary of the input to achieve a specific output size.
Why we not padding is that sometimes we we get a very small activation map on the matrix thus losing information on the original image

Having pads gives us a larger activation map coverage thus more information and accuracy

have to flatten the 2x2 matrix into a long vector matrix and then apply the mlp.

Will have multiple hidden layers and output layer depending on the classes.
Learn about convolution, filters, kernal_size.

Max pooling 2d
flatten
- changes 2d activation layer into 1d
---
MNIST popular dataset
- don't have to use pandas anymore

We can just use keras.dataset.object.

if we use onehotencoding from sklearn there will be a memory overflow so we use to_categorical.
MLP - will only have 1 hidden layer.
Training will take vey long
Might take an hour.
https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/

  

---

Normalization is not necessary for the CNN, and there are no missing values.

  

To do:

  

- convert the class labels into a one-hot encoding.

  

- preferable in an automated pipeline, testing on google colab indicates that this results in an out-of-memory error.

  

- design and parameterize the CNN in Tensorflow / Keras using the training dataset,

  - evaluate your final, best design on the testd dataset

    - evalaute using a 5-fold stratified cross-validation design for exploring your hyperparameters in the training set

  

- one you have determined the best design after doing hyperparameter exploration, train the CNN one more time on the entire 60-000 image training set,

  - then report your performance on the out-of-sample test

---
### [[LeNet Architecture]]